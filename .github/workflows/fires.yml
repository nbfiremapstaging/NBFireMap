name: Fetch ERD Fire Datasets

on:
  schedule:
    # Runs at 21 minutes after every hour (UTC)
    - cron: "21 * * * *"
  workflow_dispatch: {}

jobs:
  fetch-and-commit:
    runs-on: ubuntu-latest
    env:
      PYTHONUNBUFFERED: "1"
      ACTIVE_FIRES_URL: "https://gis-erd-der.gnb.ca/arcgis/rest/services/Fire_Dashboards/Public_Fires/MapServer/0/query"
      OUT_FIRES_URL: "https://gis-erd-der.gnb.ca/arcgis/rest/services/Fire_Dashboards/Public_Fires/MapServer/1/query"
      SUMS_TABLE_URL: "https://gis-erd-der.gnb.ca/arcgis/rest/services/Fire_Dashboards/Public_Fires/MapServer/2/query"
      ACTIVE_FIRES_FILE: "active_fires.geojson"
      OUT_FIRES_FILE: "out_fires.geojson"
      SUMS_TABLE_FILE: "sums_table.json"
      # PDF → JSON
      GNB_PDF_URL: "https://www3.gnb.ca/public/fire-feu/activitysum_e.pdf"
      GNB_SUM_FILE: "GNBfireActSum.json"

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.x"

      - name: Install dependencies
        run: |
          set -euo pipefail
          python -m pip install --upgrade pip
          pip install requests pdfplumber

      - name: Fetch ERD layers, validate, and write files (temp script)
        shell: bash
        run: |
          set -euo pipefail

          cat > fetch_erd.py <<'PY'
import os, time, json, hashlib, requests

SESSION = requests.Session()
SESSION.headers.update({"User-Agent": "github-actions ERD fetcher"})

def log(msg):
    ts = time.strftime("%Y-%m-%d %H:%M:%S", time.gmtime())
    print(f"[{ts} UTC] {msg}", flush=True)

def arcgis_get(url, params, *, attempts=3, timeout=60):
    for attempt in range(1, attempts + 1):
        try:
            r = SESSION.get(url, params=params, timeout=timeout)
            if r.status_code == 200:
                return r
            log(f"HTTP {r.status_code} from {url} (attempt {attempt})")
        except Exception as e:
            log(f"Request error (attempt {attempt}): {e}")
        time.sleep(1)
    raise RuntimeError(f"Failed after {attempts} attempts: {url}")

# ---------- Schema helpers ----------
def fetch_layer_meta(url):
    meta_url = url.rsplit("/query", 1)[0]
    r = arcgis_get(meta_url, {"f": "json"})
    meta = r.json()
    if not isinstance(meta, dict):
        raise RuntimeError("Layer metadata not a dict")
    return meta

def get_oid_field(meta):
    if isinstance(meta.get("objectIdField"), str):
        return meta["objectIdField"]
    for f in meta.get("fields", []):
        if f.get("type") == "esriFieldTypeOID":
            return f.get("name")
    for cand in ("OBJECTID","ObjectID","objectid"):
        if any(cand == f.get("name") for f in meta.get("fields", [])):
            return cand
    return "OBJECTID"

def get_all_field_names(meta):
    return [f["name"] for f in meta.get("fields", []) if "name" in f]

# ---------- Signatures for stability ----------
def features_to_signature(feature_collection):
    feats = feature_collection.get("features", [])
    def feature_key(f):
        props = f.get("properties", {})
        if isinstance(props, dict):
            for key in ("OBJECTID","ObjectID","objectid"):
                if key in props:
                    return (0, props[key])
        return (1, hashlib.sha256(json.dumps(props, sort_keys=True, ensure_ascii=False).encode("utf-8")).hexdigest())
    norm = []
    for f in feats:
        props = f.get("properties", {}) or {}
        strings = []
        for k, v in props.items():
            if isinstance(v, str):
                strings.append(v)
        strings.sort()
        norm.append(("".join(strings), feature_key(f)))
    norm.sort(key=lambda t: t[1])
    concat = "|".join(s for s, _ in norm)
    return len(feats), hashlib.sha256(concat.encode("utf-8")).hexdigest()

def table_to_signature(table_json):
    feats = table_json.get("features", [])
    rows = []
    for f in feats:
        attrs = f.get("attributes", {}) or {}
        strings = []
        for k, v in attrs.items():
            if isinstance(v, str):
                strings.append(v)
        strings.sort()
        rows.append("".join(strings))
    rows.sort()
    concat = "|".join(rows)
    return len(feats), hashlib.sha256(concat.encode("utf-8")).hexdigest()

# ---------- Fetchers ----------
def fetch_geojson(url):
    params = {"where":"1=1","outFields":"*","outSR":"4326","returnGeometry":"true","f":"geojson"}
    r = arcgis_get(url, params)
    data = r.json()
    if isinstance(data, dict) and data.get("error"):
        raise RuntimeError(f"ArcGIS error (geojson): {data['error']}")
    if data.get("type") != "FeatureCollection":
        raise RuntimeError("GeoJSON did not contain a FeatureCollection")
    return data

def attributes_index_by_oid(attrs_json, oid_field):
    index = {}
    for f in attrs_json.get("features", []):
        attrs = f.get("attributes") or {}
        if oid_field in attrs:
            index[attrs[oid_field]] = attrs
    return index

# ---------- Merge ----------
def merge_geojson_with_attrs(geojson_obj, attrs_by_oid, all_fields, oid_field):
    for f in geojson_obj.get("features", []):
        props = f.get("properties")
        if props is None or not isinstance(props, dict):
            props = {}
            f["properties"] = props
        oid_val = props.get(oid_field)
        source = attrs_by_oid.get(oid_val, {})
        for name in all_fields:
            if name not in props or props[name] is None:
                if name in source:
                    props[name] = source[name]
                else:
                    if name not in props:
                        props[name] = None
    return geojson_obj

def add_timestamp_to_geojson(data, timestamp):
    feats = data.get("features", [])
    for f in feats:
        if "properties" not in f or not isinstance(f["properties"], dict):
            f["properties"] = {}
        f["properties"]["FETCHED_FROM_ERD"] = int(timestamp)
    return data

def add_timestamp_to_table(data, timestamp):
    feats = data.get("features", [])
    for f in feats:
        attrs = f.get("attributes")
        if attrs is None or not isinstance(attrs, dict):
            f["attributes"] = {}
            attrs = f["attributes"]
        attrs["FETCHED_FROM_ERD"] = int(timestamp)
    return data

# ---------- Orchestrated fetch ----------
def attempt_fetch_geojson_with_full_attrs(kind, url, outfile, copies=3):
    meta = fetch_layer_meta(url)
    oid_field = get_oid_field(meta)
    all_fields = get_all_field_names(meta)
    log(f"{kind}: OID field='{oid_field}', fields={len(all_fields)}")

    payloads = []
    sigs = []
    for i in range(copies):
        base_geo = fetch_geojson(url)
        attrs_json_resp = arcgis_get(url, {"where":"1=1","outFields":"*","returnGeometry":"false","f":"json"}).json()
        attrs_by_oid = attributes_index_by_oid(attrs_json_resp, oid_field)
        merged = merge_geojson_with_attrs(base_geo, attrs_by_oid, all_fields, oid_field)
        count, sig = features_to_signature(merged)
        payloads.append(merged)
        sigs.append((count, sig))
        log(f"{kind}: copy {i+1}/{copies} -> features={count} sig={sig[:12]}")
        time.sleep(1)

    # Write first copy; they typically match but we don't fail if not
    ts = int(time.time())
    final = add_timestamp_to_geojson(payloads[0], ts)
    with open(outfile, "w", encoding="utf-8") as f:
        json.dump(final, f, ensure_ascii=False, indent=2)
    log(f"{kind}: wrote {outfile} (features={sigs[0][0]})")
    return True

def attempt_fetch_table_json(kind, url, outfile, copies=3):
    payloads = []
    sigs = []
    for i in range(copies):
        params = {"where":"1=1","outFields":"*","returnGeometry":"false","f":"json"}
        r = arcgis_get(url, params)
        data = r.json()
        if isinstance(data, dict) and data.get("error"):
            raise RuntimeError(f"ArcGIS error for {kind}: {data['error']}")
        payloads.append(data)
        count, sig = table_to_signature(data)
        sigs.append((count, sig))
        log(f"{kind}: copy {i+1}/{copies} -> rows={count} sig={sig[:12]}")
        time.sleep(1)

    ts = int(time.time())
    final = add_timestamp_to_table(payloads[0], ts)
    with open(outfile, "w", encoding="utf-8") as f:
        json.dump(final, f, ensure_ascii=False, indent=2)
    log(f"{kind}: wrote {outfile} (rows={sigs[0][0]})")
    return True

def main():
    ok = True
    ok &= attempt_fetch_geojson_with_full_attrs(
        kind="active_fires (layer 0)",
        url=os.environ["ACTIVE_FIRES_URL"],
        outfile=os.environ["ACTIVE_FIRES_FILE"]
    )
    ok &= attempt_fetch_geojson_with_full_attrs(
        kind="out_fires (layer 1)",
        url=os.environ["OUT_FIRES_URL"],
        outfile=os.environ["OUT_FIRES_FILE"]
    )
    ok &= attempt_fetch_table_json(
        kind="sums_table (layer 2)",
        url=os.environ["SUMS_TABLE_URL"],
        outfile=os.environ["SUMS_TABLE_FILE"]
    )
    if ok:
        log("All datasets fetched successfully.")
    else:
        log("Some datasets failed.")

if __name__ == "__main__":
    main()
PY

          python fetch_erd.py

      - name: Fetch & parse Activity Summary PDF → GNBfireActSum.json (temp script)
        shell: bash
        run: |
          set -euo pipefail

          cat > parse_pdf.py <<'PY'
import os, io, time, json, re, requests, pdfplumber

pdf_url = os.environ["GNB_PDF_URL"]
out_json = os.environ["GNB_SUM_FILE"]
ts = int(time.time())

def clean(s):
    if s is None: return ""
    return re.sub(r"\s+", " ", str(s)).strip()

result = {
    "source": pdf_url,
    "fetched_utc": ts,
    "pages": 0,
    "tables": []
}

r = requests.get(pdf_url, timeout=90)
r.raise_for_status()
data = io.BytesIO(r.content)

with pdfplumber.open(data) as pdf:
    result["pages"] = len(pdf.pages)

    # First attempt: true tables
    for pnum, page in enumerate(pdf.pages, start=1):
        tables = page.extract_tables(
            table_settings={
                "vertical_strategy": "lines",
                "horizontal_strategy": "lines",
                "intersection_y_tolerance": 2,
                "intersection_x_tolerance": 2,
                "snap_tolerance": 3,
                "min_words_vertical": 1,
                "min_words_horizontal": 1,
                "text_x_tolerance": 2,
                "text_y_tolerance": 2,
                "join_tolerance": 3,
            }
        ) or []
        for t in tables:
            if not t or len(t) < 2:
                continue
            header = [clean(x) for x in t[0]]
            keep_idx = [i for i, h in enumerate(header) if h]
            header = [header[i] for i in keep_idx]
            rows_obj = []
            for row in t[1:]:
                row = row or []
                row_clean = [clean(x) for x in row]
                row_keep = [row_clean[i] if i < len(row_clean) else "" for i in keep_idx]
                if len(row_keep) < len(header):
                    row_keep += [""] * (len(header) - len(row_keep))
                elif len(row_keep) > len(header):
                    row_keep = row_keep[:len(header)]
                if any(v for v in row_keep):
                    rows_obj.append(dict(zip(header, row_keep)))
            if rows_obj:
                result["tables"].append({"page": pnum, "headers": header, "rows": rows_obj})

    # Fallback: robust text parser tailored to this PDF
    if not result["tables"]:
        headers = ["Region","Status","Number","Fire Name","Updated","Latitude","Longitude","Size","Pers","Eng","Tend","Trac","Air","Heli","Ovr"]
        rows = []
        region_totals = []
        province_total = None

        status_opts = [
            "Out of Control",
            "Under Control",
            "Being Patrolled",
            "Contained",
            "Out",
        ]

        def starts_with_status(line):
            for s in status_opts:
                if line.startswith(s + " "):
                    return s, line[len(s):].strip()
            return None, line

        def parse_data_line(line, current_region):
            status, rest = starts_with_status(line)
            if not status:
                return None
            parts = rest.split()
            if len(parts) < 1:
                return None
            if not parts[0].isdigit():
                return None
            num = parts[0]
            parts = parts[1:]

            date_idx = None
            for i, tok in enumerate(parts):
                if re.fullmatch(r"[A-Za-z]{3}-\d{1,2}", tok):
                    date_idx = i
                    break
            if date_idx is None or date_idx + 8 >= len(parts):
                return None

            fire_name = " ".join(parts[:date_idx])
            updated = parts[date_idx]
            tail = parts[date_idx+1:]

            lat, lon, size = tail[0], tail[1], tail[2]
            ints = tail[3:3+7]  # Pers Eng Tend Trac Air Heli Ovr
            if len(ints) != 7:
                return None

            return {
                "Region": current_region or "",
                "Status": status,
                "Number": num,
                "Fire Name": fire_name,
                "Updated": updated,
                "Latitude": lat,
                "Longitude": lon,
                "Size": size,
                "Pers": ints[0],
                "Eng": ints[1],
                "Tend": ints[2],
                "Trac": ints[3],
                "Air": ints[4],
                "Heli": ints[5],
                "Ovr": ints[6],
            }

        def try_region_total(line):
            m = re.match(r"^Total for (.+?) Region\s+([0-9,]+(?:\.\d+)?)\s+(\d+)\s+(\d+)\s+(\d+)\s+(\d+)\s+(\d+)\s+(\d+)$", line)
            if not m:
                return None
            return {
                "Region": m.group(1),
                "Size": m.group(2),
                "Pers": m.group(3),
                "Eng": m.group(4),
                "Tend": m.group(5),
                "Trac": m.group(6),
                "Air": m.group(7),
                "HeliOvr": m.group(8)
            }

        def try_province_total(line):
            m = re.match(r"^Total for Province\s+([0-9,]+(?:\.\d+)?)\s+(\d+)\s+(\d+)\s+(\d+)\s+(\d+)\s+(\d+)\s+(\d+)$", line)
            if not m:
                return None
            return {
                "Size": m.group(1),
                "Pers": m.group(2),
                "Eng": m.group(3),
                "Tend": m.group(4),
                "Trac": m.group(5),
                "Air": m.group(6),
                "HeliOvr": m.group(7)
            }

        for page in pdf.pages:
            text = page.extract_text() or ""
            lines = [clean(x) for x in text.splitlines() if clean(x)]
            current_region = ""
            for line in lines:
                if re.fullmatch(r".+ Region", line):
                    current_region = line.replace(" Region","")
                    continue
                rt = try_region_total(line)
                if rt:
                    region_totals.append(rt)
                    continue
                pt = try_province_total(line)
                if pt:
                    province_total = pt
                    continue
                row = parse_data_line(line, current_region)
                if row:
                    rows.append(row)

        if rows:
            result["tables"].append({
                "page": 0,
                "headers": headers,
                "rows": rows
            })
        if region_totals:
            result["region_totals"] = region_totals
        if province_total:
            result["province_total"] = province_total

with open(out_json, "w", encoding="utf-8") as f:
    json.dump(result, f, ensure_ascii=False, indent=2)

print(f"Wrote {out_json}. Tables: {len(result.get('tables', []))}; rows: {sum(len(t['rows']) for t in result.get('tables', [])) if result.get('tables') else 0}")
PY

          python parse_pdf.py

      - name: Commit and push (if changed)
        shell: bash
        run: |
          set -euo pipefail
          if [ -n "$(git status --porcelain)" ]; then
            echo "Changes detected; committing…"
            git config user.name "github-actions[bot]"
            git config user.email "github-actions[bot]@users.noreply.github.com"
            git add "${ACTIVE_FIRES_FILE}" "${OUT_FIRES_FILE}" "${SUMS_TABLE_FILE}" "${GNB_SUM_FILE}" || true
            git commit -m "Update ERD fire datasets (incl. Activity Summary PDF → JSON)" -m "Run: $(date -u +'%Y-%m-%dT%H:%M:%SZ')"
            git push
            echo "Pushed updates."
          else
            echo "No changes to commit."
          fi
